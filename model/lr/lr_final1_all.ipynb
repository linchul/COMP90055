{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee52034",
   "metadata": {},
   "source": [
    "# TFIDF-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf6a74",
   "metadata": {},
   "source": [
    "# 1„ÄÅdata extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145d1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e42b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_url(text):\n",
    "    '''\n",
    "    args:\n",
    "        text: the tweet text\n",
    "    return:\n",
    "        new_text: tweet text removed the url\n",
    "    '''\n",
    "    URL_REGEX = URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:\\'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    url_lists = re.findall(URL_REGEX,text)\n",
    "    for url in url_lists:\n",
    "        new_text = text.replace(url,'')\n",
    "        text = new_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff67898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangyibo06/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tt = TweetTokenizer(strip_handles=True)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# tt = TweetTokenizer(strip_handles=True)\n",
    "def lower_word(data):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data:  text while text include source text and reply text\n",
    "        \n",
    "    return:\n",
    "        result: text while text include source text and reply text(remove the stopwords)\n",
    "    \"\"\"\n",
    "    res = ''\n",
    "    for word in tt.tokenize(data):\n",
    "        if word.lower().strip() in stopwords:\n",
    "            continue\n",
    "        elif word.lower().startswith(\"#\"):\n",
    "            res += ' hashtag_'+word.lower()[1:] \n",
    "        elif word.lower().startswith(\"@\") and len(word) > 1:\n",
    "            res += ' att_'+word.lower()[1:] \n",
    "        elif word.lower().startswith(\"@\") and len(word) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            res += ' '+word.lower()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7890d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filename):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data: {user_id:{\"text\":text,\"user\":user,\"place_id\":place_id}}\n",
    "     \n",
    "    return:\n",
    "        result: {text: [text1,text2,...],length:[length1,length2,...],label:[rumor or not,...] } \n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as obj:\n",
    "        for line in obj.readlines():\n",
    "            data = json.loads(line)\n",
    "    result = {}\n",
    "    text = []\n",
    "    textlabel = []\n",
    "    location = []\n",
    "    length = []\n",
    "    for user_id in data.keys():\n",
    "        text.append(lower_word(remove_url(data[user_id]['user']['location']+\" \"+data[user_id]['text'])))\n",
    "        textlabel.append(data[user_id]['place_id']) \n",
    "        length.append(len(lower_word(remove_url(data[user_id]['user']['location']+\" \"+data[user_id]['text']))))\n",
    "        location.append(data[user_id]['user']['location'])\n",
    "    result['text'] = text\n",
    "    result['length'] = length\n",
    "    result['label'] = textlabel\n",
    "    result['location'] = location\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9119ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './train_dev_data/0905_1005.txt'\n",
    "train_dev = prepare_data(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f337e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f393d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>label</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>west end , brisbane posted photo west end</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>West End, Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>melbourne , australia posted photo caulfield ...</td>\n",
       "      <td>1555</td>\n",
       "      <td>3</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sydney , new south wales watercolour art clas...</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sydney , new south wales posted photo centenn...</td>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bishopsbourne , tasmania posted photo richmon...</td>\n",
       "      <td>154</td>\n",
       "      <td>11</td>\n",
       "      <td>Bishopsbourne, Tasmania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  length  label  \\\n",
       "0          west end , brisbane posted photo west end      42      4   \n",
       "1   melbourne , australia posted photo caulfield ...    1555      3   \n",
       "2   sydney , new south wales watercolour art clas...     194      1   \n",
       "3   sydney , new south wales posted photo centenn...     867      1   \n",
       "4   bishopsbourne , tasmania posted photo richmon...     154     11   \n",
       "\n",
       "                  location  \n",
       "0       West End, Brisbane  \n",
       "1     Melbourne, Australia  \n",
       "2  Sydney, New South Wales  \n",
       "3  Sydney, New South Wales  \n",
       "4  Bishopsbourne, Tasmania  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dev_df = pd.DataFrame(train_dev)\n",
    "train_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abc6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train_dev dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_processed = train_dev_df['text']\n",
    "y_processed = train_dev_df['label']\n",
    "x_train,x_dev,y_train,y_dev = train_test_split(x_processed,y_processed,test_size = 0.3,stratify = y_processed,random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b33875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2078\n",
      "2132\n"
     ]
    }
   ],
   "source": [
    "print(len(set(x_train)))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0bc503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n",
      "915\n",
      "121     1\n",
      "1473    0\n",
      "1741    4\n",
      "2189    5\n",
      "2666    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_dev))\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f320347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train.iloc[:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af0bb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f162ca07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_n: 2 features: 10000 score:0.4684\n",
      "max_n: 2 features: 20000 score:0.4527\n",
      "max_n: 3 features: 10000 score:0.4802\n",
      "max_n: 3 features: 20000 score:0.4493\n",
      "max_n: 4 features: 10000 score:0.4851\n",
      "max_n: 4 features: 20000 score:0.4586\n",
      "max_n: 5 features: 10000 score:0.4902\n",
      "max_n: 5 features: 20000 score:0.4659\n",
      "best_n: 5 best_features: 10000 best_score:0.4902\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "#lr ngram 4,features 10000\n",
    "from sklearn import svm\n",
    "\n",
    "best_score = 0\n",
    "best_clf = None\n",
    "best_tfidf = None\n",
    "for max_n in range(2,6):\n",
    "    for features in range(10000,30000,10000):\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, max_n), max_features=features).fit(x_train.iloc[:].values)\n",
    "        train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "        dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "        val_pred = clf.predict(dev_tfidf)\n",
    "        score = f1_score(y_dev.iloc[:].values, val_pred, average='macro')\n",
    "        print('max_n:',max_n,'features:',features,'score:%.4f'%score)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_clf = clf\n",
    "            best_max_n = max_n\n",
    "            best_features = features\n",
    "print('best_n:',best_max_n,'best_features:',best_features,'best_score:%.4f'%best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd0f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_n: 5 features: 1000 score:0.4659\n",
      "max_n: 5 features: 2000 score:0.4758\n",
      "max_n: 5 features: 3000 score:0.4908\n",
      "max_n: 5 features: 4000 score:0.4967\n",
      "max_n: 5 features: 5000 score:0.5033\n",
      "max_n: 5 features: 6000 score:0.5042\n",
      "max_n: 5 features: 7000 score:0.5014\n",
      "max_n: 5 features: 8000 score:0.4923\n",
      "max_n: 5 features: 9000 score:0.5035\n",
      "max_n: 5 features: 10000 score:0.4902\n",
      "max_n: 5 features: 11000 score:0.4828\n",
      "max_n: 5 features: 12000 score:0.4805\n",
      "max_n: 5 features: 13000 score:0.4831\n",
      "max_n: 5 features: 14000 score:0.4826\n",
      "max_n: 5 features: 15000 score:0.4802\n",
      "max_n: 5 features: 16000 score:0.4782\n",
      "max_n: 5 features: 17000 score:0.4747\n",
      "max_n: 5 features: 18000 score:0.4652\n",
      "max_n: 5 features: 19000 score:0.4673\n",
      "best_n: 5 best_features: 6000 best_score:0.5042\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "#lr ngram 4,features 10000\n",
    "from sklearn import svm\n",
    "\n",
    "best_score = 0\n",
    "best_clf = None\n",
    "best_tfidf = None\n",
    "for max_n in range(5,6):\n",
    "    for features in range(1000,20000,1000):\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, max_n), max_features=features).fit(x_train.iloc[:].values)\n",
    "        train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "        dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "        val_pred = clf.predict(dev_tfidf)\n",
    "        score = f1_score(y_dev.iloc[:].values, val_pred, average='macro')\n",
    "        print('max_n:',max_n,'features:',features,'score:%.4f'%score)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_clf = clf\n",
    "            best_max_n = max_n\n",
    "            best_features = features\n",
    "print('best_n:',best_max_n,'best_features:',best_features,'best_score:%.4f'%best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c86254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_n: 5 best_features: 6000 best_score:0.5042\n"
     ]
    }
   ],
   "source": [
    "print('best_n:',best_max_n,'best_features:',best_features,'best_score:%.4f'%best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c77a8b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf-lr_ngram5_features4000_score0.40955266149151254.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# store the model\n",
    "joblib.dump(best_clf, 'tfidf-lr_ngram{}_features{}_score{}.pkl'.format(best_max_n,best_features,best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4e9e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_n = 3\n",
    "best_features = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "585c2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score:0.6699\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.10      0.18        20\n",
      "           0       0.77      0.39      0.52        69\n",
      "           1       0.60      0.90      0.72       214\n",
      "           2       0.67      0.32      0.43        38\n",
      "           3       0.71      0.87      0.78       187\n",
      "           4       0.67      0.63      0.65        79\n",
      "           5       0.62      0.61      0.62        95\n",
      "           6       0.00      0.00      0.00        13\n",
      "           7       0.73      0.65      0.69        51\n",
      "           8       0.80      0.68      0.74        60\n",
      "           9       1.00      0.18      0.31        22\n",
      "          10       0.75      0.43      0.55         7\n",
      "          11       0.80      0.36      0.50        11\n",
      "          12       0.50      0.32      0.39        22\n",
      "          13       1.00      0.50      0.67         6\n",
      "          14       0.93      0.65      0.76        20\n",
      "          15       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67       915\n",
      "   macro avg       0.68      0.45      0.50       915\n",
      "weighted avg       0.69      0.67      0.64       915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangyibo06/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/wangyibo06/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/wangyibo06/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,best_max_n+1), max_features=best_features).fit(x_train.iloc[:].values)\n",
    "train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "\n",
    "val_pred = clf.predict(dev_tfidf)\n",
    "# score = f1_score(y_dev.iloc[:].values, val_pred, average='macro')\n",
    "score = f1_score(y_dev.iloc[:].values, val_pred, average='micro')\n",
    "\n",
    "print('best_score:%.4f'%score)\n",
    "print(metrics.classification_report(y_dev.iloc[:].values,val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4022b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare for the test data\n",
    "def prepare_test(filename):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data: {user_id:{\"text\":text,\"user\":user,\"place_id\":place_id}}\n",
    "     \n",
    "    return:\n",
    "        result: {text: [text1,text2,...],length:[length1,length2,...],label:[rumor or not,...] } \n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as obj:\n",
    "        for line in obj.readlines():\n",
    "            data = json.loads(line)\n",
    "    result = {}\n",
    "    text = []\n",
    "    location = []\n",
    "    length = []\n",
    "    user_ids = []\n",
    "    for user_id in data.keys():\n",
    "        user_ids.append(user_id)\n",
    "        text.append(data[user_id]['text'])\n",
    "        length.append(len(data[user_id]['text']))\n",
    "        location.append(data[user_id]['location'])\n",
    "    result['user_id'] = user_ids\n",
    "    result['text'] = text\n",
    "    result['length'] = length\n",
    "    result['location'] = location\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aae4041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './test_data/0905.txt'\n",
    "test = prepare_test(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3236ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276090111</td>\n",
       "      <td>@aVoice2bHrd You know how I am üò≠I cut family o...</td>\n",
       "      <td>138</td>\n",
       "      <td>West Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1499908182</td>\n",
       "      <td>@AusAndy Most people probably have one now fro...</td>\n",
       "      <td>187</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1952211163</td>\n",
       "      <td>@anassilvvaa Bora@anassilvvaa Se envolver dinh...</td>\n",
       "      <td>1948</td>\n",
       "      <td>Invicta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>788532504626012160</td>\n",
       "      <td>@60Mins Will this program be replayed?Is last ...</td>\n",
       "      <td>1284</td>\n",
       "      <td>Newcastle, New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1459159765</td>\n",
       "      <td>Breaking news: emergency COVID laws applying t...</td>\n",
       "      <td>133</td>\n",
       "      <td>Fortitude Valley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id                                               text  \\\n",
       "0           276090111  @aVoice2bHrd You know how I am üò≠I cut family o...   \n",
       "1          1499908182  @AusAndy Most people probably have one now fro...   \n",
       "2          1952211163  @anassilvvaa Bora@anassilvvaa Se envolver dinh...   \n",
       "3  788532504626012160  @60Mins Will this program be replayed?Is last ...   \n",
       "4          1459159765  Breaking news: emergency COVID laws applying t...   \n",
       "\n",
       "   length                    location  \n",
       "0     138                  West Coast  \n",
       "1     187     Sydney, New South Wales  \n",
       "2    1948                     Invicta  \n",
       "3    1284  Newcastle, New South Wales  \n",
       "4     133            Fortitude Valley  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6704d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 ... 3 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,best_max_n+1), max_features=best_features).fit(x_train.iloc[:].values)\n",
    "# train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "# dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "test_tfidf = tfidf.transform(test_df['text'].iloc[:].values)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "\n",
    "val_pred = clf.predict(test_tfidf)\n",
    "print(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d905d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>location</th>\n",
       "      <th>predict_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276090111</td>\n",
       "      <td>@aVoice2bHrd You know how I am üò≠I cut family o...</td>\n",
       "      <td>138</td>\n",
       "      <td>West Coast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1499908182</td>\n",
       "      <td>@AusAndy Most people probably have one now fro...</td>\n",
       "      <td>187</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1952211163</td>\n",
       "      <td>@anassilvvaa Bora@anassilvvaa Se envolver dinh...</td>\n",
       "      <td>1948</td>\n",
       "      <td>Invicta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>788532504626012160</td>\n",
       "      <td>@60Mins Will this program be replayed?Is last ...</td>\n",
       "      <td>1284</td>\n",
       "      <td>Newcastle, New South Wales</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1459159765</td>\n",
       "      <td>Breaking news: emergency COVID laws applying t...</td>\n",
       "      <td>133</td>\n",
       "      <td>Fortitude Valley</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id                                               text  \\\n",
       "0           276090111  @aVoice2bHrd You know how I am üò≠I cut family o...   \n",
       "1          1499908182  @AusAndy Most people probably have one now fro...   \n",
       "2          1952211163  @anassilvvaa Bora@anassilvvaa Se envolver dinh...   \n",
       "3  788532504626012160  @60Mins Will this program be replayed?Is last ...   \n",
       "4          1459159765  Breaking news: emergency COVID laws applying t...   \n",
       "\n",
       "   length                    location  predict_label  \n",
       "0     138                  West Coast              1  \n",
       "1     187     Sydney, New South Wales              3  \n",
       "2    1948                     Invicta              1  \n",
       "3    1284  Newcastle, New South Wales              3  \n",
       "4     133            Fortitude Valley              1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#construct the test output file\n",
    "test_df['predict_label'] = val_pred\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe3e2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write predictions to json\n",
    "# from collection import OrderedDict\n",
    "def write2json(filename,dataframe):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        filename: the filename of the predicted data label file\n",
    "        dataframe: the dataframe of the predicted data\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_cnt = 0\n",
    "    new_dict = {}\n",
    "    with open(filename, 'w') as file:\n",
    "        \n",
    "        for index,row in dataframe.iterrows():\n",
    "            new_dict['id'] = row['user_id']\n",
    "            new_dict['place_id'] = row['predict_label']\n",
    "        \n",
    "            json_line = json.dumps(new_dict)\n",
    "            file.write(json_line+'\\n')\n",
    "            file_cnt += 1\n",
    "            if file_cnt % 100 == 0:\n",
    "                print('file:'+str(file_cnt))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d3db04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:100\n",
      "file:200\n",
      "file:300\n",
      "file:400\n",
      "file:500\n",
      "file:600\n",
      "file:700\n",
      "file:800\n",
      "file:900\n",
      "file:1000\n",
      "file:1100\n",
      "file:1200\n",
      "file:1300\n",
      "file:1400\n",
      "file:1500\n",
      "file:1600\n",
      "file:1700\n",
      "file:1800\n",
      "file:1900\n",
      "file:2000\n",
      "file:2100\n",
      "file:2200\n",
      "file:2300\n",
      "file:2400\n",
      "file:2500\n",
      "file:2600\n",
      "file:2700\n",
      "file:2800\n",
      "file:2900\n",
      "file:3000\n",
      "file:3100\n",
      "file:3200\n",
      "file:3300\n",
      "file:3400\n",
      "file:3500\n",
      "file:3600\n",
      "file:3700\n",
      "file:3800\n",
      "file:3900\n",
      "file:4000\n",
      "file:4100\n",
      "file:4200\n",
      "file:4300\n",
      "file:4400\n",
      "file:4500\n",
      "file:4600\n",
      "file:4700\n",
      "file:4800\n",
      "file:4900\n",
      "file:5000\n",
      "file:5100\n",
      "file:5200\n",
      "file:5300\n",
      "file:5400\n",
      "file:5500\n",
      "file:5600\n",
      "file:5700\n",
      "file:5800\n",
      "file:5900\n",
      "file:6000\n",
      "file:6100\n",
      "file:6200\n",
      "file:6300\n",
      "file:6400\n",
      "file:6500\n",
      "file:6600\n",
      "file:6700\n",
      "file:6800\n",
      "file:6900\n",
      "file:7000\n",
      "file:7100\n",
      "file:7200\n",
      "file:7300\n",
      "file:7400\n",
      "file:7500\n",
      "file:7600\n",
      "file:7700\n",
      "file:7800\n",
      "file:7900\n",
      "file:8000\n",
      "file:8100\n",
      "file:8200\n",
      "file:8300\n",
      "file:8400\n",
      "file:8500\n",
      "file:8600\n",
      "file:8700\n",
      "file:8800\n",
      "file:8900\n",
      "file:9000\n",
      "file:9100\n",
      "file:9200\n",
      "file:9300\n",
      "file:9400\n",
      "file:9500\n",
      "file:9600\n",
      "file:9700\n",
      "file:9800\n",
      "file:9900\n",
      "file:10000\n",
      "file:10100\n",
      "file:10200\n",
      "file:10300\n",
      "file:10400\n",
      "file:10500\n",
      "file:10600\n",
      "file:10700\n",
      "file:10800\n",
      "file:10900\n",
      "file:11000\n",
      "file:11100\n",
      "file:11200\n",
      "file:11300\n",
      "file:11400\n",
      "file:11500\n",
      "file:11600\n",
      "file:11700\n",
      "file:11800\n",
      "file:11900\n",
      "file:12000\n",
      "file:12100\n",
      "file:12200\n",
      "file:12300\n",
      "file:12400\n",
      "file:12500\n",
      "file:12600\n",
      "file:12700\n",
      "file:12800\n",
      "file:12900\n",
      "file:13000\n",
      "file:13100\n",
      "file:13200\n",
      "file:13300\n",
      "file:13400\n",
      "file:13500\n",
      "file:13600\n",
      "file:13700\n",
      "file:13800\n",
      "file:13900\n",
      "file:14000\n",
      "file:14100\n",
      "file:14200\n",
      "file:14300\n",
      "file:14400\n",
      "file:14500\n",
      "file:14600\n",
      "file:14700\n",
      "file:14800\n",
      "file:14900\n",
      "file:15000\n",
      "file:15100\n",
      "file:15200\n",
      "file:15300\n",
      "file:15400\n",
      "file:15500\n",
      "file:15600\n",
      "file:15700\n",
      "file:15800\n",
      "file:15900\n",
      "file:16000\n",
      "file:16100\n",
      "file:16200\n",
      "file:16300\n",
      "file:16400\n",
      "file:16500\n",
      "file:16600\n",
      "file:16700\n",
      "file:16800\n",
      "file:16900\n",
      "file:17000\n",
      "file:17100\n",
      "file:17200\n",
      "file:17300\n",
      "file:17400\n",
      "file:17500\n",
      "file:17600\n",
      "file:17700\n",
      "file:17800\n",
      "file:17900\n",
      "file:18000\n",
      "file:18100\n",
      "file:18200\n",
      "file:18300\n",
      "file:18400\n",
      "file:18500\n",
      "file:18600\n",
      "file:18700\n",
      "file:18800\n",
      "file:18900\n",
      "file:19000\n",
      "file:19100\n",
      "file:19200\n",
      "file:19300\n",
      "file:19400\n",
      "file:19500\n",
      "file:19600\n",
      "file:19700\n",
      "file:19800\n",
      "file:19900\n",
      "file:20000\n",
      "file:20100\n",
      "file:20200\n",
      "file:20300\n",
      "file:20400\n",
      "file:20500\n",
      "file:20600\n",
      "file:20700\n",
      "file:20800\n",
      "file:20900\n",
      "file:21000\n",
      "file:21100\n",
      "file:21200\n",
      "file:21300\n",
      "file:21400\n",
      "file:21500\n",
      "file:21600\n",
      "file:21700\n",
      "file:21800\n",
      "file:21900\n",
      "file:22000\n",
      "file:22100\n",
      "file:22200\n",
      "file:22300\n",
      "file:22400\n",
      "file:22500\n",
      "file:22600\n",
      "file:22700\n",
      "file:22800\n",
      "file:22900\n",
      "file:23000\n",
      "file:23100\n",
      "file:23200\n",
      "file:23300\n",
      "file:23400\n",
      "file:23500\n",
      "file:23600\n",
      "file:23700\n",
      "file:23800\n",
      "file:23900\n",
      "file:24000\n",
      "file:24100\n",
      "file:24200\n",
      "file:24300\n",
      "file:24400\n",
      "file:24500\n",
      "file:24600\n",
      "file:24700\n",
      "file:24800\n",
      "file:24900\n",
      "file:25000\n",
      "file:25100\n",
      "file:25200\n",
      "file:25300\n",
      "file:25400\n",
      "file:25500\n",
      "file:25600\n",
      "file:25700\n",
      "file:25800\n",
      "file:25900\n",
      "file:26000\n",
      "file:26100\n",
      "file:26200\n",
      "file:26300\n",
      "file:26400\n",
      "file:26500\n",
      "file:26600\n",
      "file:26700\n",
      "file:26800\n",
      "file:26900\n",
      "file:27000\n",
      "file:27100\n",
      "file:27200\n",
      "file:27300\n",
      "file:27400\n",
      "file:27500\n",
      "file:27600\n",
      "file:27700\n",
      "file:27800\n",
      "file:27900\n",
      "file:28000\n",
      "file:28100\n",
      "file:28200\n",
      "file:28300\n",
      "file:28400\n",
      "file:28500\n",
      "file:28600\n",
      "file:28700\n",
      "file:28800\n",
      "file:28900\n",
      "file:29000\n",
      "file:29100\n",
      "file:29200\n",
      "file:29300\n",
      "file:29400\n",
      "file:29500\n",
      "file:29600\n",
      "file:29700\n",
      "file:29800\n",
      "file:29900\n",
      "file:30000\n",
      "file:30100\n",
      "file:30200\n",
      "file:30300\n",
      "file:30400\n",
      "file:30500\n",
      "file:30600\n",
      "file:30700\n",
      "file:30800\n",
      "file:30900\n",
      "file:31000\n",
      "file:31100\n",
      "file:31200\n",
      "file:31300\n",
      "file:31400\n",
      "file:31500\n",
      "file:31600\n",
      "file:31700\n",
      "file:31800\n",
      "file:31900\n",
      "file:32000\n",
      "file:32100\n",
      "file:32200\n",
      "file:32300\n",
      "file:32400\n",
      "file:32500\n",
      "file:32600\n",
      "file:32700\n",
      "file:32800\n",
      "file:32900\n",
      "file:33000\n",
      "file:33100\n",
      "file:33200\n",
      "file:33300\n",
      "file:33400\n",
      "file:33500\n",
      "file:33600\n",
      "file:33700\n",
      "file:33800\n",
      "file:33900\n",
      "file:34000\n",
      "file:34100\n",
      "file:34200\n",
      "file:34300\n",
      "file:34400\n",
      "file:34500\n",
      "file:34600\n",
      "file:34700\n",
      "file:34800\n",
      "file:34900\n",
      "file:35000\n",
      "file:35100\n",
      "file:35200\n",
      "file:35300\n",
      "file:35400\n",
      "file:35500\n",
      "file:35600\n",
      "file:35700\n",
      "file:35800\n",
      "file:35900\n",
      "file:36000\n",
      "file:36100\n",
      "file:36200\n",
      "file:36300\n",
      "file:36400\n",
      "file:36500\n",
      "file:36600\n",
      "file:36700\n",
      "file:36800\n",
      "file:36900\n",
      "file:37000\n",
      "file:37100\n",
      "file:37200\n",
      "file:37300\n",
      "file:37400\n",
      "file:37500\n",
      "file:37600\n",
      "file:37700\n",
      "file:37800\n",
      "file:37900\n",
      "file:38000\n",
      "file:38100\n",
      "file:38200\n",
      "file:38300\n",
      "file:38400\n",
      "file:38500\n",
      "file:38600\n",
      "file:38700\n",
      "file:38800\n",
      "file:38900\n",
      "file:39000\n",
      "file:39100\n",
      "file:39200\n",
      "file:39300\n",
      "file:39400\n",
      "file:39500\n",
      "file:39600\n",
      "file:39700\n",
      "file:39800\n",
      "file:39900\n",
      "file:40000\n",
      "file:40100\n",
      "file:40200\n",
      "file:40300\n",
      "file:40400\n",
      "file:40500\n",
      "file:40600\n",
      "file:40700\n",
      "file:40800\n",
      "file:40900\n",
      "file:41000\n",
      "file:41100\n",
      "file:41200\n",
      "file:41300\n",
      "file:41400\n",
      "file:41500\n",
      "file:41600\n",
      "file:41700\n",
      "file:41800\n",
      "file:41900\n",
      "file:42000\n",
      "file:42100\n",
      "file:42200\n",
      "file:42300\n",
      "file:42400\n",
      "file:42500\n",
      "file:42600\n",
      "file:42700\n",
      "file:42800\n",
      "file:42900\n",
      "file:43000\n",
      "file:43100\n",
      "file:43200\n",
      "file:43300\n",
      "file:43400\n",
      "file:43500\n",
      "file:43600\n",
      "file:43700\n",
      "file:43800\n",
      "file:43900\n",
      "file:44000\n",
      "file:44100\n",
      "file:44200\n",
      "file:44300\n",
      "file:44400\n",
      "file:44500\n",
      "file:44600\n",
      "file:44700\n",
      "file:44800\n",
      "file:44900\n",
      "file:45000\n",
      "file:45100\n",
      "file:45200\n",
      "file:45300\n",
      "file:45400\n",
      "file:45500\n",
      "file:45600\n",
      "file:45700\n",
      "file:45800\n",
      "file:45900\n",
      "file:46000\n",
      "file:46100\n",
      "file:46200\n",
      "file:46300\n",
      "file:46400\n",
      "file:46500\n",
      "file:46600\n",
      "file:46700\n",
      "file:46800\n",
      "file:46900\n",
      "file:47000\n",
      "file:47100\n",
      "file:47200\n",
      "file:47300\n",
      "file:47400\n",
      "file:47500\n",
      "file:47600\n",
      "file:47700\n",
      "file:47800\n",
      "file:47900\n",
      "file:48000\n",
      "file:48100\n",
      "file:48200\n",
      "file:48300\n",
      "file:48400\n",
      "file:48500\n",
      "file:48600\n",
      "file:48700\n",
      "file:48800\n",
      "file:48900\n",
      "file:49000\n",
      "file:49100\n",
      "file:49200\n",
      "file:49300\n",
      "file:49400\n",
      "file:49500\n",
      "file:49600\n",
      "file:49700\n",
      "file:49800\n",
      "file:49900\n",
      "file:50000\n",
      "file:50100\n",
      "file:50200\n",
      "file:50300\n",
      "file:50400\n",
      "file:50500\n",
      "file:50600\n",
      "file:50700\n",
      "file:50800\n",
      "file:50900\n",
      "file:51000\n",
      "file:51100\n",
      "file:51200\n",
      "file:51300\n",
      "file:51400\n",
      "file:51500\n",
      "file:51600\n",
      "file:51700\n",
      "file:51800\n",
      "file:51900\n",
      "file:52000\n",
      "file:52100\n",
      "file:52200\n",
      "file:52300\n",
      "file:52400\n",
      "file:52500\n",
      "file:52600\n",
      "file:52700\n",
      "file:52800\n",
      "file:52900\n",
      "file:53000\n",
      "file:53100\n",
      "file:53200\n",
      "file:53300\n",
      "file:53400\n",
      "file:53500\n",
      "file:53600\n",
      "file:53700\n",
      "file:53800\n",
      "file:53900\n",
      "file:54000\n",
      "file:54100\n",
      "file:54200\n",
      "file:54300\n",
      "file:54400\n",
      "file:54500\n",
      "file:54600\n",
      "file:54700\n",
      "file:54800\n",
      "file:54900\n",
      "file:55000\n",
      "file:55100\n",
      "file:55200\n",
      "file:55300\n",
      "file:55400\n",
      "file:55500\n",
      "file:55600\n",
      "file:55700\n",
      "file:55800\n",
      "file:55900\n",
      "file:56000\n",
      "file:56100\n",
      "file:56200\n",
      "file:56300\n",
      "file:56400\n",
      "file:56500\n",
      "file:56600\n",
      "file:56700\n",
      "file:56800\n",
      "file:56900\n",
      "file:57000\n",
      "file:57100\n",
      "file:57200\n",
      "file:57300\n",
      "file:57400\n",
      "file:57500\n",
      "file:57600\n",
      "file:57700\n",
      "file:57800\n",
      "file:57900\n",
      "file:58000\n",
      "file:58100\n",
      "file:58200\n",
      "file:58300\n",
      "file:58400\n",
      "file:58500\n",
      "file:58600\n",
      "file:58700\n",
      "file:58800\n",
      "file:58900\n",
      "file:59000\n",
      "file:59100\n",
      "file:59200\n",
      "file:59300\n",
      "file:59400\n",
      "file:59500\n",
      "file:59600\n",
      "file:59700\n",
      "file:59800\n",
      "file:59900\n",
      "file:60000\n",
      "file:60100\n",
      "file:60200\n",
      "file:60300\n",
      "file:60400\n",
      "file:60500\n",
      "file:60600\n",
      "file:60700\n",
      "file:60800\n",
      "file:60900\n",
      "file:61000\n",
      "file:61100\n",
      "file:61200\n",
      "file:61300\n",
      "file:61400\n",
      "file:61500\n",
      "file:61600\n",
      "file:61700\n",
      "file:61800\n",
      "file:61900\n",
      "file:62000\n"
     ]
    }
   ],
   "source": [
    "write2json('./ml_predict/lr/lr_0905_v10.json',test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473a50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
